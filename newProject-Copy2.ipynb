{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets,transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['cardboard','fabrics','plastic','poly','wet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_dir = 'Cat_Dog_data/train'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                                transforms.CenterCrop(128),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/train/', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cardboard': 0, 'fabrics': 1, 'plastic': 2, 'poly': 3, 'wet': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 128x128x3 image tensor)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, padding=2)\n",
    "        self.bn1=nn.BatchNorm2d(16)\n",
    "        # convolutional layer (sees 64x64x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2=nn.BatchNorm2d(32)\n",
    "        # convolutional layer (sees 32x32x32 tensor)\n",
    "        self.conv3 = nn.Conv2d(32, 48, 3, padding=1)\n",
    "                                #sees 16*16*48\n",
    "        self.bn3=nn.BatchNorm2d(48)\n",
    "        self.conv4 = nn.Conv2d(48, 24, 3, padding=1)\n",
    "                                #sees 8*8*24\n",
    "        self.bn4=nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(24, 12, 8, padding=0)\n",
    "        \n",
    "        #output 1*1*6\n",
    "        \n",
    "        \n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (64 * 4 * 4 -> 500)\n",
    "        self.fc1 = nn.Linear(12,8)\n",
    "        self.fc2=nn.Linear(8,5)\n",
    "        self.drop=nn.Dropout(p=0.30)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.bn1(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x=self.bn2(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x=self.bn3(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x=self.bn4(x)\n",
    "        x = self.conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1,1*1*12)\n",
    "        x = self.drop(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.003)\n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                                transforms.CenterCrop(128),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/testing/', transform=transform)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(24, 12, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12, out_features=8, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=5, bias=True)\n",
       "  (drop): Dropout(p=0.3)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "state=torch.load('copy2.pt')\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist=[]\n",
    "validlist=[]\n",
    "epochlist=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_min =np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848555749793751"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \tTraining Loss: 0.127022 \n",
      "Epoch: 36 \tvalidation Loss: 0.136020 \n",
      "Epoch: 37 \tTraining Loss: 0.088250 \n",
      "Epoch: 37 \tvalidation Loss: 0.508232 \n",
      "Epoch: 38 \tTraining Loss: 0.096229 \n",
      "Epoch: 38 \tvalidation Loss: 0.072675 \n",
      "Validation loss decreased (0.120343 --> 0.072675).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.095682 \n",
      "Epoch: 39 \tvalidation Loss: 0.054935 \n",
      "Validation loss decreased (0.072675 --> 0.054935).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.077435 \n",
      "Epoch: 40 \tvalidation Loss: 0.054591 \n",
      "Validation loss decreased (0.054935 --> 0.054591).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "\n",
    "n_epochs = 40 \n",
    "\n",
    " # track change in validation loss\n",
    "\n",
    "for epoch in range(36, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss=0.0\n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        #print(data.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        #target = target -1\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        #if train_on_gpu:\n",
    "            #data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(dataloader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    trainlist.append(train_loss)\n",
    "    validlist.append(valid_loss)\n",
    "    epochlist.append(epoch)\n",
    "    #valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} '.format(\n",
    "        epoch, train_loss))\n",
    "    print('Epoch: {} \\tvalidation Loss: {:.6f} '.format(\n",
    "        epoch, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'copy2.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2vfile = open(\"c2vfile\",\"a\");\n",
    "c2vfile.write(str(validlist))\n",
    "c2tfile = open(\"c2tfile\",\"a\");\n",
    "c2tfile.write(str(trainlist))\n",
    "c2efile = open(\"c2efile\",\"a\");\n",
    "c2efile.write(str(epochlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochlist,trainlist,'r',label='train_error')\n",
    "plt.plot(epochlist,validlist,'b',label='validation_error')\n",
    "plt.legend(loc='best')\n",
    "ax=plt.gca()\n",
    "ax.set_ylim([0,3])\n",
    "ax.set_xlim([0,1600])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 4, 2, 4, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochlist,trainlist,'r',label='train_error')\n",
    "plt.plot(epochlist,validlist,'b',label='validation_error')\n",
    "plt.legend(loc='best')\n",
    "ax=plt.gca()\n",
    "ax.set_ylim([0,7])\n",
    "ax.set_xlim([0,40])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 5])\n",
      "{'cardboard': 0, 'fabrics': 1, 'plastic': 2, 'poly': 3, 'wet': 4}\n",
      "ground truth is  tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
      "prediction is    tensor([0, 0, 0, 3, 2, 2, 1, 4, 0, 1, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 2, 3, 4, 4, 2, 0, 2, 0, 2, 2, 2, 4, 2, 2, 2, 2])\n",
      "tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       dtype=torch.uint8)\n",
      "accuracy=  61.702126264572144\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(128),\n",
    "                                transforms.CenterCrop(128),\n",
    "                                transforms.RandomHorizontalFlip(p=0.0),                                \n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/realtest/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=50, shuffle=False)\n",
    "model.eval()\n",
    "for data,target in test_loader:\n",
    "    output = model(data)\n",
    "    #break;\n",
    "output=torch.exp(output)\n",
    "print(output.shape)\n",
    "_,output=torch.max(output,dim=1)\n",
    "print(dataset.class_to_idx)\n",
    "print(\"ground truth is \",target)\n",
    "print(\"prediction is   \",output)\n",
    "\n",
    "print(target==output)\n",
    "#print(float(torch.sum(target==output).type('torch.FloatTensor')/2.0))\n",
    "print(\"accuracy= \",float(torch.sum(target==output).type('torch.FloatTensor')/len(target==output))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "raw output is  tensor([[2.2429, 1.1435, 1.0256, 1.0180, 1.0152],\n",
      "        [1.1876, 1.4967, 1.0958, 1.1102, 1.2569],\n",
      "        [1.4740, 1.0222, 1.4862, 1.0459, 1.1606],\n",
      "        [1.0241, 1.8155, 1.0051, 1.3173, 1.1043]], grad_fn=<ExpBackward>)\n",
      "{'cardboard': 0, 'fabrics': 1, 'plastic': 2, 'poly': 3, 'wet': 4}\n",
      "ground truth is  tensor([3])\n",
      "prediction is    tensor([0, 1, 2, 1])\n",
      "accuracy=  0.0\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(144),\n",
    "                                transforms.CenterCrop(144),\n",
    "                                transforms.RandomHorizontalFlip(p=0.0),                                \n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/real/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=50, shuffle=False)\n",
    "model.eval()\n",
    "for data,target in test_loader:\n",
    "    output = model(data)\n",
    "    #break;\n",
    "output=torch.exp(output)\n",
    "print(output.shape)\n",
    "print(\"raw output is \",torch.exp(output))\n",
    "_,output=torch.max(output,dim=1)\n",
    "\n",
    "print(dataset.class_to_idx)\n",
    "print(\"ground truth is \",target)\n",
    "print(\"prediction is   \",output)\n",
    "\n",
    "#print(target==output)\n",
    "#print(float(torch.sum(target==output).type('torch.FloatTensor')/2.0))\n",
    "print(\"accuracy= \",float(torch.sum(target==output).type('torch.FloatTensor')/len(target==output))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2378, 5])\n",
      "raw output is  tensor([[1.0074, 1.0361, 1.0710, 2.4261, 1.0023],\n",
      "        [1.0002, 1.0083, 1.4258, 1.8894, 1.0005],\n",
      "        [2.7072, 1.0009, 1.0030, 1.0000, 1.0001],\n",
      "        ...,\n",
      "        [1.0023, 1.0104, 1.0040, 1.0531, 2.5388],\n",
      "        [1.4321, 1.3667, 1.1654, 1.1861, 1.0047],\n",
      "        [1.6410, 1.2296, 1.2612, 1.0606, 1.0071]], grad_fn=<ExpBackward>)\n",
      "{'cardboard': 0, 'fabrics': 1, 'plastic': 2, 'poly': 3, 'wet': 4}\n",
      "ground truth is  tensor([3])\n",
      "prediction is    tensor([3, 3, 0,  ..., 4, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "transform = transforms.Compose([#transforms.Resize(144),#                                                    \n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/real1/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "for data,target in test_loader:\n",
    "    output = model(data)\n",
    "    #break;\n",
    "output=torch.exp(output)\n",
    "print(output.shape)\n",
    "print(\"raw output is \",torch.exp(output))\n",
    "_,output=torch.max(output,dim=1)\n",
    "\n",
    "print(dataset.class_to_idx)\n",
    "print(\"ground truth is \",target)\n",
    "print(\"prediction is   \",output)\n",
    "#import cv2\n",
    "#img=cv2.imread('C:/Users/Shahnawaz/Desktop/testing/poly/blak.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([                                                     \n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder('C:/Users/Shahnawaz/Desktop/real1/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenser=torch.from_numpy(img)\n",
    "tenser=tenser.unsqueeze(0)\n",
    "#tenser=tenser.permute(0,3,1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 3, 5, 5], expected input[1, 128, 128, 3] to have 3 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-cd2c261a7342>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtenser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtenser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'torch.FloatTensor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtenser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-e17e734f7081>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# add sequence of convolutional and max pooling layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 5, 5], expected input[1, 128, 128, 3] to have 3 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "tenser = tenser.type('torch.FloatTensor')\n",
    "output=model(tenser)\n",
    "output=torch.exp(output)\n",
    "_,output=torch.max(output,dim=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0.]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
